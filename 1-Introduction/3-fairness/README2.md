# Fairness in ml üßë‚Äç‚öñÔ∏è
When I want to create an ai project I must be careful to consider all groups, diversity, ethnicity, gender, religion so that I don't create an application that is racist, insenstitive or does not cater for all groups of people.

### Classification of Fairness related harms
<ul>
  <li>Allocation </li>  <li>Denigration</li>  <li> Quality of service </li>  <li> Streyotyping </li> <li> Under or over representation </li>
</ul>

note that all these things are also known as bias in data.

> Allocation is when a gender or race is favoured


> Quality of service: If you train the data for one specific scenario and reality is more complex than that so the application is unable to cater for all other scenarios. a guy posted on twitter that he could not make use of the dispenser because it could not recognize his hands


> Streyotyping (categorizing): Assigning a given race with a particular attributes, e.g black race is known for violence so Ai is trained to percieve black people as violent example is in coded bias documentary. Example is if you translate "he is a nurse", "she is a doctor" in turkish and then translate it to english it would be "he is a doctor" "she is a nurse"


> Denigration: unfairly critizing a group


> over or under representation: it is a joke that black peole don't swim, so imagine someone creates an ai app that does not allow a black person to register. The idea that a certain group is not seen a certain profession.

### Consequences of not considering other groups
In  The Impact of Artificial Intelligence on Women‚Äôs Rights: A Legal Point of View by Cecilia Celeste Danesi, ppg 220 from zaynab's perspective it seems like history may repeat itself like in the case of the luddites. Apart from the cases mentioned above, what of the people that ai will take thier jobs away from them because they are not skilled enough.
2. If someone applies for a loan and the person i rejected because of thier skin color
3. if a woman applies for a role and her application is turned down because the ai was trained with data as a male domainated field etc
4. if a black person is labelled as a criminal
5. if a black person is unable to make use of services and equipments because it does not cater for them.

> üí≠ if I make use of real datasets and the datasets include biases which are by the way real, if I correct them then I have as well altered the datasets, is it not better to make use of the datasets the way they are which is also the way the world works. 
> <i> Note </i> this illustration above is for some bias such as steryotype, 


### Detecting Unfairness
1. wrong assumptions
2. inadequate data

Solution
